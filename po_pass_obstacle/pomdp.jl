using POMDPs, SARSOP, POMDPSimulators
using POMDPTools
using Infiltrator
include("MDP.jl")

# Constants
stop_reliability = 0.85
edge_reliability = 0.75
go_reliability = 0.65
# function POPOMDP(M::DomainSSP)
#     return POPOMDP(M)
# end

## Initialize Model Parameters
W = get_random_world_state()
M = build_model(W)

# Generate domain states
S = M.S

# Generate domain actions
A = M.A

# POMDP Transition function
function POMDPs.transition(ğ’«::POMDP, state::DomainState, action::DomainAction)
    s, a = ğ’«.M.SIndex[state], ğ’«.M.AIndex[action]
    return SparseCat(S, ğ’«.M.T[s][a])
end

# POMDP Reward function
function POMDPs.reward(ğ’«::POMDP, state::DomainState, action::DomainAction)
    s, a = ğ’«.M.SIndex[state], ğ’«.M.AIndex[action]
    return ğ’«.M.R[s][a]
end

# POMDP Observation
struct Observation
    trailing::Bool
    oncoming::Int
end

# Generate observation set O
OIndex = Dict{Observation, Integer}()
function build_observations()
    Î© = Vector{Observation}()
    index_ = 1
    for t=0:1
        for o=-1:3
            Ï‰ = Observation(t, o)
            push!(Î©, Ï‰)
            OIndex[Ï‰] = index_
            index_ += 1
        end
    end
    return Î©
end
Î© = build_observations()
function index(Ï‰::Observation)
    return OIndex[Ï‰]
end

# Generate observation function Î©
function generate_observations()
    O = Dict{Int, Dict{Int, SparseCat}}()
    for (a, action) in enumerate(A)
        O[a] = Dict{Int, SparseCat}()
        for (sp, statePrime) in enumerate(S)
            if action.value == :stop
                P = Vector{Float64}()
                for Ï‰ âˆˆ Î©
                    p = 1.0

                    # Trailing observation
                    if Ï‰.trailing == statePrime.w.trailing
                        p *= stop_reliability
                    else
                        p *= (1.0 - stop_reliability)
                    end

                    # Oncoming observation
                    if Ï‰.oncoming == statePrime.oncoming == -1
                        p *= 1.0
                    elseif Ï‰.oncoming == statePrime.oncoming
                        p *= stop_reliability
                    elseif Ï‰.oncoming == -1
                        p *= (1.0 - stop_reliability)
                    else
                        p *= 0.0
                    end
                    push!(P, p)
                end
                O[a][sp] = SparseCat(Î©, P)
            elseif action.value == :edge
                P = Vector{Float64}()
                for Ï‰ âˆˆ Î©
                    p = 1.0

                    # Trailing observation
                    if Ï‰.trailing == statePrime.w.trailing
                        p *= edge_reliability
                    else
                        p *= (1.0 - edge_reliability)
                    end

                    # Oncoming observation
                    if Ï‰.oncoming == statePrime.oncoming == -1
                        p *= 1.0
                    elseif Ï‰.oncoming == statePrime.oncoming
                        p *= edge_reliability
                    elseif Ï‰.oncoming == -1
                        p *= (1.0 - edge_reliability)
                    else
                        p *= 0.0
                    end

                    push!(P, p)
                end
                O[a][sp] = SparseCat(Î©, P)
            else
                P = Vector{Float64}()
                for Ï‰ âˆˆ Î©
                    p = 1.0

                    # Trailing observation
                    if Ï‰.oncoming == statePrime.oncoming == -1
                        p *= 1.0
                    elseif Ï‰.trailing == statePrime.w.trailing
                        p *= go_reliability
                    else
                        p *= (1.0 - go_reliability)
                    end

                    # Oncoming observation
                    if Ï‰.oncoming == statePrime.oncoming
                        p *= go_reliability
                    elseif Ï‰.oncoming == -1
                        p *= (1.0 - go_reliability)
                    else
                        p *= 0.0
                    end

                    push!(P, p)
                end
                O[a][sp] = SparseCat(Î©, P)
            end
        end
    end
    return O
end
O = generate_observations()
function POMDPs.observation(ğ’«::POMDP, action::DomainAction, state::DomainState)
    a, sp = ğ’«.M.AIndex[action], ğ’«.M.SIndex[state]
    return ğ’«.O[a][sp]
end

struct POPOMDP <: POMDP{DomainState, DomainAction, Observation}
    M::MDP
    O::Dict{Int, Dict{Int, SparseCat}}
    OIndex::Dict{Observation, Int}
end

POMDPs.states(ğ’«::POPOMDP) = S
POMDPs.actions(ğ’«::POPOMDP) = A
POMDPs.observations(ğ’«::POPOMDP) = Î©
POMDPs.isterminal(ğ’«::POPOMDP, state::DomainState) = terminal(ğ’«.M, state)
POMDPs.discount(ğ’«::POPOMDP) = 0.9
POMDPs.initialstate(ğ’«::POPOMDP) = Deterministic(ğ’«.M.sâ‚€)
POMDPs.stateindex(ğ’«::POPOMDP, state::DomainState) = ğ’«.M.SIndex[state]
POMDPs.actionindex(ğ’«::POPOMDP, action::DomainAction) = ğ’«.M.AIndex[action]
POMDPs.obsindex(ğ’«::POPOMDP, Ï‰::Observation) = ğ’«.OIndex[Ï‰]

ğ’« = POPOMDP(M, O, OIndex)

# =================== SOLVER CONFIGURATION ===================

@time begin
    solver = SARSOPSolver()
    policy = @time SARSOP.solve(solver, ğ’«)
end

begin
    rsum = 0.0
    rewards = Vector{Float64}()

    @time for i=1:100
        global rsum = 0.0
        for (s,b,a,o,r) in stepthrough(ğ’«, policy, "s,b,a,o,r", max_steps=1000)
            println("s: $s, a: $a, o: $o")
            r = POMDPs.reward(ğ’«, s, a)
            global rsum += r
        end
        push!(rewards, rsum)
    end

    println("Average reward: $(mean(rewards)) Â± $(std(rewards))")
    println(rewards)
end
